import webrtcvad
import collections
import pyaudio
import os
import wave
import threading
import queue
import time
import numpy as np
from collections import deque


class WebRTCVADRecorder:

    def __init__(self, rate=16000, aggressiveness=2):
        self.rate = rate
        self.vad = webrtcvad.Vad(aggressiveness)  # 0-3ï¼Œ3æœ€æ¿€è¿›

        # å‚æ•°é…ç½®
        self.frame_duration = 30  # æ¯«ç§’ï¼Œwebrtcvadè¦æ±‚10,20,30ms
        self.chunk = int(rate * self.frame_duration / 1000)  # æ¯å¸§é‡‡æ ·æ•°
        self.silence_timeout = 2.0  # é™éŸ³è¶…æ—¶ï¼ˆç§’ï¼‰
        self.min_recording_duration = 1.0  # æœ€å°å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰

        self.format = pyaudio.paInt16
        self.channels = 1

    def record_until_silence(self, filename="user_audio.wav"):
        """ä½¿ç”¨WebRTC VADè¿›è¡Œæ™ºèƒ½å½•éŸ³"""
        p = pyaudio.PyAudio()

        try:
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk,
                input_device_index=1
            )

            print("å¼€å§‹æ™ºèƒ½å½•éŸ³ï¼ˆWebRTC VADï¼‰...")
            frames = []
            voiced_frames = []
            ring_buffer = collections.deque(maxlen=10)  # ç”¨äºå¹³æ»‘æ£€æµ‹

            silence_frames_threshold = int(self.silence_timeout * 1000 / self.frame_duration)
            min_frames = int(self.min_recording_duration * 1000 / self.frame_duration)

            silence_frames = 0
            total_frames = 0
            is_recording = False

            while True:
                data = stream.read(self.chunk, exception_on_overflow=False)

                # ä½¿ç”¨VADæ£€æµ‹è¯­éŸ³æ´»åŠ¨
                try:
                    is_speech = self.vad.is_speech(data, self.rate)
                except:
                    is_speech = False

                ring_buffer.append(1 if is_speech else 0)

                # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ¤æ–­ï¼ˆå‡å°‘è¯¯åˆ¤ï¼‰
                speech_ratio = sum(ring_buffer) / len(ring_buffer)
                is_voiced = speech_ratio > 0.5

                if is_voiced:
                    if not is_recording:
                        print("æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
                        is_recording = True
                    silence_frames = 0
                    voiced_frames.append(data)
                else:
                    silence_frames += 1

                # å¦‚æœå·²ç»å¼€å§‹å½•éŸ³ï¼Œä¿å­˜æ‰€æœ‰å¸§
                if is_recording:
                    frames.append(data)
                    total_frames += 1

                    # æ£€æŸ¥åœæ­¢æ¡ä»¶
                    if (silence_frames >= silence_frames_threshold and
                            len(voiced_frames) >= min_frames):
                        duration = total_frames * self.frame_duration / 1000
                        print(f"æ£€æµ‹åˆ°æŒç»­é™éŸ³ï¼Œåœæ­¢å½•éŸ³ã€‚å½•éŸ³æ—¶é•¿: {duration:.2f}ç§’")
                        break

                # å®‰å…¨é™åˆ¶
                if total_frames > (30 * 1000 / self.frame_duration):  # 30ç§’
                    print("è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿ï¼Œè‡ªåŠ¨åœæ­¢")
                    break

            stream.stop_stream()
            stream.close()

            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            if frames:
                wf = wave.open(filename, 'wb')
                wf.setnchannels(self.channels)
                wf.setsampwidth(p.get_sample_size(self.format))
                wf.setframerate(self.rate)
                wf.writeframes(b''.join(frames))
                wf.close()

                file_size = os.path.getsize(filename)
                print(f"æ™ºèƒ½å½•éŸ³å®Œæˆ: {filename}, å¤§å°: {file_size} å­—èŠ‚")
                return filename
            else:
                print("æ²¡æœ‰å½•åˆ¶åˆ°éŸ³é¢‘")
                return None

        except Exception as e:
            print(f"æ™ºèƒ½å½•éŸ³è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None
        finally:
            p.terminate()


class RealTimeVoiceMonitor:
    def __init__(self, companion_instance, rate=16000, chunk=480,
                 silence_threshold=200, min_silence_duration=1.5,
                 max_single_utterance=10.0):
        """
        å®æ—¶è¯­éŸ³ç›‘æ§å™¨

        Args:
            companion_instance: é™ªä¼´æœºå™¨äººå®ä¾‹
            rate: é‡‡æ ·ç‡
            chunk: æ¯æ¬¡è¯»å–çš„éŸ³é¢‘å—å¤§å°ï¼ˆæ¨è480ï¼Œå¯¹åº”30msï¼Œé€‚åˆVADï¼‰
            silence_threshold: é™éŸ³é˜ˆå€¼
            min_silence_duration: æœ€å°é™éŸ³æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            max_single_utterance: å•æ¬¡è¯´è¯æœ€å¤§æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        self.companion = companion_instance
        self.rate = rate
        self.chunk = chunk
        self.silence_threshold = silence_threshold
        self.min_silence_duration = min_silence_duration
        self.max_single_utterance = max_single_utterance

        self.format = pyaudio.paInt16
        self.channels = 1

        # çŠ¶æ€æ§åˆ¶
        self.is_listening = False
        self.is_processing = False
        self.current_state = "idle"  # idle, detecting, recording, processing

        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = deque(maxlen=int(rate * 2 / chunk))  # ä¿å­˜2ç§’éŸ³é¢‘ç”¨äºé¢„è§¦å‘
        self.current_recording = []

        # çº¿ç¨‹å’Œé˜Ÿåˆ—
        self.audio_queue = queue.Queue()
        self.processing_queue = queue.Queue()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_detections": 0,
            "processed_utterances": 0,
            "last_activity_time": time.time()
        }

    def calculate_energy(self, audio_data):
        """è®¡ç®—éŸ³é¢‘èƒ½é‡ - ç¨³å®šç‰ˆæœ¬"""
        try:
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            if len(audio_array) == 0:
                return 0

            # ä½¿ç”¨ç»å¯¹å€¼çš„å¹³å‡å€¼ï¼Œé¿å…å¹³æ–¹æ ¹é—®é¢˜
            # è¿™ç§æ–¹æ³•å¯¹é™éŸ³å¸§æ›´ç¨³å®šï¼Œè®¡ç®—å¼€é”€ä¹Ÿæ›´å°
            energy = np.mean(np.abs(audio_array))
            return energy

        except Exception as e:
            print(f"è®¡ç®—èƒ½é‡æ—¶å‡ºé”™: {e}")
            return 0

    def auto_calibrate_threshold(self, calibration_seconds=3):
        """è‡ªåŠ¨æ ¡å‡†é™éŸ³é˜ˆå€¼"""
        print("æ­£åœ¨è¿›è¡Œç¯å¢ƒå™ªéŸ³æ ¡å‡†ï¼Œè¯·ä¿æŒå®‰é™...")

        p = pyaudio.PyAudio()
        try:
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk,
                input_device_index=1
            )

            energies = []
            total_frames = int(self.rate * calibration_seconds / self.chunk)

            for i in range(total_frames):
                data = stream.read(self.chunk, exception_on_overflow=False)
                energy = self.calculate_energy(data)
                energies.append(energy)

            avg_energy = np.mean(energies)
            std_energy = np.std(energies)

            # è®¾ç½®é˜ˆå€¼ä¸ºå¹³å‡å€¼åŠ ä¸Š2å€æ ‡å‡†å·®
            new_threshold = avg_energy + 2 * std_energy
            # ç¡®ä¿é˜ˆå€¼è‡³å°‘ä¸º400
            new_threshold = max(400, new_threshold)

            print(f"ç¯å¢ƒå™ªéŸ³æ°´å¹³: {avg_energy:.2f} Â± {std_energy:.2f}")
            print(f"è‡ªåŠ¨è®¾ç½®é™éŸ³é˜ˆå€¼: {new_threshold:.2f}")

            self.silence_threshold = new_threshold

            stream.stop_stream()
            stream.close()
            return new_threshold

        except Exception as e:
            print(f"æ ¡å‡†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None
        finally:
            p.terminate()

    def audio_capture_thread(self):
        """éŸ³é¢‘æ•è·çº¿ç¨‹ - æŒç»­ä»éº¦å…‹é£è¯»å–æ•°æ®"""
        p = pyaudio.PyAudio()

        try:
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk,
                input_device_index=1
            )

            print("éŸ³é¢‘æ•è·çº¿ç¨‹å¯åŠ¨ - å¼€å§‹æŒç»­ç›‘å¬...")

            while self.is_listening:
                try:
                    data = stream.read(self.chunk, exception_on_overflow=False)
                    self.audio_queue.put(data)
                except Exception as e:
                    print(f"éŸ³é¢‘æ•è·é”™è¯¯: {e}")
                    time.sleep(0.1)

            stream.stop_stream()
            stream.close()

        except Exception as e:
            print(f"éŸ³é¢‘æ•è·çº¿ç¨‹é”™è¯¯: {e}")
        finally:
            p.terminate()

    def voice_detection_thread(self):
        """è¯­éŸ³æ£€æµ‹çº¿ç¨‹ - å®æ—¶æ£€æµ‹è¯­éŸ³æ´»åŠ¨"""
        print("è¯­éŸ³æ£€æµ‹çº¿ç¨‹å¯åŠ¨ - ç­‰å¾…è¯­éŸ³æ´»åŠ¨...")

        silence_frames_threshold = int(self.min_silence_duration * self.rate / self.chunk)
        max_recording_frames = int(self.max_single_utterance * self.rate / self.chunk)

        silence_frames = 0
        is_recording = False
        recording_start_time = 0
        recording_frames = 0

        # é¢„è§¦å‘ç¼“å†²åŒºï¼ˆä¿å­˜è¯­éŸ³å¼€å§‹å‰0.5ç§’çš„éŸ³é¢‘ï¼‰
        pre_trigger_buffer = deque(maxlen=int(0.5 * self.rate / self.chunk))

        while self.is_listening:
            try:
                # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘æ•°æ®ï¼ˆé˜»å¡ï¼Œæœ€å¤šç­‰å¾…100msï¼‰
                data = self.audio_queue.get(timeout=0.1)

                # æ›´æ–°é¢„è§¦å‘ç¼“å†²åŒº
                pre_trigger_buffer.append(data)

                # è®¡ç®—èƒ½é‡
                energy = self.calculate_energy(data)

                if is_recording:
                    # å½•éŸ³çŠ¶æ€
                    self.current_recording.append(data)
                    recording_frames += 1

                    # æ£€æŸ¥èƒ½é‡æ°´å¹³
                    if energy <= self.silence_threshold:
                        silence_frames += 1
                    else:
                        silence_frames = 0

                    # æ£€æŸ¥åœæ­¢æ¡ä»¶
                    current_time = time.time()
                    recording_duration = current_time - recording_start_time

                    # æ¡ä»¶1: æŒç»­é™éŸ³è¶…è¿‡é˜ˆå€¼
                    condition1 = silence_frames >= silence_frames_threshold
                    # æ¡ä»¶2: è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿
                    condition2 = recording_duration >= self.max_single_utterance
                    # æ¡ä»¶3: å½•éŸ³å¸§æ•°è¿‡å¤š
                    condition3 = recording_frames >= max_recording_frames

                    if condition1 or condition2 or condition3:
                        # åœæ­¢å½•éŸ³ï¼Œå‡†å¤‡å¤„ç†
                        print(f"è¯­éŸ³ç»“æŸï¼Œå½•éŸ³æ—¶é•¿: {recording_duration:.2f}ç§’")

                        # å°†å®Œæ•´çš„å½•éŸ³æ•°æ®æ”¾å…¥å¤„ç†é˜Ÿåˆ—
                        full_recording = list(pre_trigger_buffer) + self.current_recording
                        self.processing_queue.put(full_recording)

                        # é‡ç½®çŠ¶æ€
                        is_recording = False
                        self.current_recording = []
                        silence_frames = 0
                        recording_frames = 0
                        self.current_state = "processing"

                    # å®æ—¶æ˜¾ç¤ºå½•éŸ³çŠ¶æ€
                    if recording_frames % 10 == 0:  # æ¯10å¸§æ˜¾ç¤ºä¸€æ¬¡
                        print(f"å½•éŸ³ä¸­... {recording_duration:.1f}ç§’, èƒ½é‡: {energy:.1f}")

                else:
                    # æ£€æµ‹çŠ¶æ€ - ç­‰å¾…è¯­éŸ³å¼€å§‹
                    if energy > self.silence_threshold:
                        # æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨ï¼Œå¼€å§‹å½•éŸ³
                        print("æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨ï¼Œå¼€å§‹å½•éŸ³...")
                        is_recording = True
                        recording_start_time = time.time()
                        self.current_state = "recording"
                        self.stats["total_detections"] += 1

                        # ä¿å­˜å½“å‰é¢„è§¦å‘ç¼“å†²åŒº
                        self.current_recording = list(pre_trigger_buffer)
                        recording_frames = len(pre_trigger_buffer)

            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                print(f"è¯­éŸ³æ£€æµ‹é”™è¯¯: {e}")
                time.sleep(0.1)

    def processing_thread(self):
        """å¤„ç†çº¿ç¨‹ - å¤„ç†æ£€æµ‹åˆ°çš„è¯­éŸ³"""
        print("å¤„ç†çº¿ç¨‹å¯åŠ¨ - ç­‰å¾…å¤„ç†ä»»åŠ¡...")

        temp_audio_dir = "temp_audio"
        os.makedirs(temp_audio_dir, exist_ok=True)

        while self.is_listening:
            try:
                # ä»å¤„ç†é˜Ÿåˆ—è·å–å½•éŸ³æ•°æ®ï¼ˆé˜»å¡ï¼‰
                audio_frames = self.processing_queue.get(timeout=1)

                if audio_frames is None:  # é€€å‡ºä¿¡å·
                    break

                print("å¼€å§‹å¤„ç†æ£€æµ‹åˆ°çš„è¯­éŸ³...")
                self.is_processing = True

                # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                timestamp = int(time.time())
                audio_filename = os.path.join(temp_audio_dir, f"utterance_{timestamp}.wav")

                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                p = pyaudio.PyAudio()
                wf = wave.open(audio_filename, 'wb')
                wf.setnchannels(self.channels)
                wf.setsampwidth(p.get_sample_size(self.format))
                wf.setframerate(self.rate)
                wf.writeframes(b''.join(audio_frames))
                wf.close()
                p.terminate()

                # è¯­éŸ³è½¬æ–‡æœ¬
                print("æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
                user_text = self.companion.speech_to_text(audio_filename)

                if user_text and isinstance(user_text, str) and len(user_text.strip()) > 0:
                    print(f"è¯†åˆ«ç»“æœ: {user_text}")

                    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
                    print("ç”Ÿæˆå›å¤...")
                    if hasattr(self.companion, 'call_bailian_api_with_memory'):
                        response_text = self.companion.call_bailian_api_with_memory(user_text)
                    else:
                        response_text = self.companion.call_bailian_api(user_text)

                    print(f"AIå›å¤: {response_text}")

                    # æ–‡æœ¬è½¬è¯­éŸ³å¹¶æ’­æ”¾
                    # print("åˆæˆè¯­éŸ³...")
                    response_audio = self.companion.text_to_speech(response_text)

                    if response_audio:
                        print("æ’­æ”¾å›å¤...")
                        self.companion.play_audio(response_audio)

                    self.stats["processed_utterances"] += 1
                else:
                    print("æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(audio_filename)
                except:
                    pass

                self.is_processing = False
                self.current_state = "idle"
                print("å¤„ç†å®Œæˆï¼Œè¿”å›ç›‘å¬çŠ¶æ€...")

            except queue.Empty:
                continue
            except Exception as e:
                print(f"å¤„ç†çº¿ç¨‹é”™è¯¯: {e}")
                self.is_processing = False
                self.current_state = "idle"

    def start_realtime_listening(self):
        """å¯åŠ¨å®æ—¶ç›‘å¬"""
        print("å¯åŠ¨å®æ—¶è¯­éŸ³ç›‘å¬ç³»ç»Ÿ...")
        print("æœºå™¨äººç°åœ¨å¤„äºæŒç»­ç›‘å¬çŠ¶æ€ï¼Œå¯ä»¥éšæ—¶è¯´è¯")
        print("æŒ‰ä¸‹ Ctrl+C åœæ­¢ç›‘å¬")
        # è‡ªåŠ¨æ ¡å‡†ç¯å¢ƒå™ªéŸ³
        self.auto_calibrate_threshold()

        self.is_listening = True
        self.current_state = "idle"

        # å¯åŠ¨å„ä¸ªçº¿ç¨‹
        threads = []

        # éŸ³é¢‘æ•è·çº¿ç¨‹
        capture_thread = threading.Thread(target=self.audio_capture_thread)
        capture_thread.daemon = True
        capture_thread.start()
        threads.append(capture_thread)

        # è¯­éŸ³æ£€æµ‹çº¿ç¨‹
        detection_thread = threading.Thread(target=self.voice_detection_thread)
        detection_thread.daemon = True
        detection_thread.start()
        threads.append(detection_thread)

        # å¤„ç†çº¿ç¨‹
        processing_thread = threading.Thread(target=self.processing_thread)
        processing_thread.daemon = True
        processing_thread.start()
        threads.append(processing_thread)

        # çŠ¶æ€æ˜¾ç¤ºçº¿ç¨‹
        status_thread = threading.Thread(target=self.status_monitor_thread)
        status_thread.daemon = True
        status_thread.start()
        threads.append(status_thread)

        try:
            # ä¸»çº¿ç¨‹ä¿æŒè¿è¡Œ
            while self.is_listening:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\næ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­å®æ—¶ç›‘å¬...")
            self.stop_realtime_listening()

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for thread in threads:
            thread.join(timeout=2)

        print("å®æ—¶ç›‘å¬ç³»ç»Ÿå·²å…³é—­")

    def status_monitor_thread(self):
        """çŠ¶æ€ç›‘æ§çº¿ç¨‹ - å®šæœŸæ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        tmp_print = ""
        while self.is_listening:
            status_info = {
                "idle": "ğŸŸ¢ ç›‘å¬ä¸­ - ç­‰å¾…è¯­éŸ³",
                "detecting": "ğŸŸ¡ æ£€æµ‹ä¸­ - åˆ†æéŸ³é¢‘",
                "recording": "ğŸ”´ å½•éŸ³ä¸­ - è¯·ç»§ç»­è¯´è¯",
                "processing": "ğŸŸ£ å¤„ç†ä¸­ - ç”Ÿæˆå›å¤"
            }

            status_emoji = status_info.get(self.current_state, "âšª æœªçŸ¥çŠ¶æ€")
            stats_text = f"æ£€æµ‹: {self.stats['total_detections']}æ¬¡, å¤„ç†: {self.stats['processed_utterances']}æ¬¡"
            now_print = f"\r{status_emoji} | {stats_text} | æŒ‰Ctrl+Cé€€å‡º"
            if now_print != tmp_print:
                tmp_print = now_print
                print(tmp_print, end="\n", flush=True)
            time.sleep(0.5)

    def stop_realtime_listening(self):
        """åœæ­¢å®æ—¶ç›‘å¬"""
        self.is_listening = False
        # å‘é€é€€å‡ºä¿¡å·åˆ°å¤„ç†é˜Ÿåˆ—
        self.processing_queue.put(None)